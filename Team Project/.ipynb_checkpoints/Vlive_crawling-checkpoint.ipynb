{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2시간 전\n",
      "6시간 전\n",
      "6시간 전\n",
      "6시간 전\n",
      "7시간 전\n",
      "7시간 전\n",
      "7시간 전\n",
      "20시간 전\n",
      "23시간 전\n",
      "2019. 11. 29. 오전 7:41\n",
      "2019. 11. 29. 오전 3:27\n",
      "2019. 11. 29. 오전 3:26\n",
      "2019. 11. 28. 오후 11:35\n",
      "2019. 11. 28. 오후 11:33\n",
      "2019. 11. 28. 오후 11:27\n",
      "2019. 11. 28. 오후 11:26\n",
      "2019. 11. 28. 오후 11:26\n",
      "2019. 11. 28. 오후 11:26\n",
      "2019. 11. 28. 오후 10:49\n",
      "2019. 11. 28. 오후 8:44\n",
      "冬が訪れましたね\n",
      "番組でEternallyがオープニングソングになって嬉しいです💓MVも少し公開されるみたいです！12月は毎週見ないとですね！！\n",
      "승희 흑발 염색 학생 연기 와우\n",
      "https://youtu.be/it5hWzmInCE\n",
      "퀸덤 이후로 브이앱 홍보했더니 오늘로 2만명 증가~\n",
      "와 키트가 도착햇네요\n",
      "https://open.kakao.com/o/gOxb1BKb\n",
      "많이 많이 들어오세요ㅎㅎ\n",
      "👍👍\n",
      "Didn't you find it hard to fall in OHMYGIRL because you are poor at Korean?\n",
      "So, we made a chat room for foreign Miracles !!\n",
      "If you have any questions about OHMYGIRL, events, etc. , Korean Miracles will explain kindly.\n",
      "Also, if you want to learn Korean, we'll help you.\n",
      "\n",
      "https://open.kakao.com/o/gJjRkaMb\n",
      "I miss you more today than yesterday! 😭❤️\n",
      "https://open.kakao.com/o/g64WKKHb\n",
      "많이 들어와주세요 ㅠㅠ\n",
      "여러분들을 위한 사진\n",
      "지호 염색머리 인사\n",
      "https://youtu.be/ug-tXXfpY2k\n",
      "와 오늘 잠안자도 되겠다\n",
      "미라클이면 다놀러와용~~ 모두친절한 미라클이랍니댱!! 예쁘고 씅청미넘치는 씅씅이~~~\n",
      "유레카!오마이걸방https://open.kakao.com/o/g6ukdKKb\n",
      "우리들은 미라클방https://open.kakao.com/o/gOxb1BKb\n",
      "Another Japanese album is coming to us on January 8, 2020. Eternally, I can’t wait to get you. 😭❤️\n",
      "하...마지막까지 설레게해..ㅜㅜ😢❤\n",
      "짧은 븨앱이지만 그래도 감사합니다\n",
      "씅 귀여워~(귀여워서 더 아쉽다 짧아 ㅠ)\n",
      "댓글 읽어줘서 고마워 ㅎㅎ\n",
      "スンヒ可愛すぎる❤\n",
      "そして、ユアからスンヒにぽっぽ可愛い❤\n",
      "永遠の私のお姫様です🙈❤️\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "\n",
    "# 오마이걸 \n",
    "url = \"https://channels.vlive.tv/F51143/fan\" \n",
    "\n",
    "# *** 드라이버 위치 포함 디렉토리로 이동 *** \n",
    "driver = webdriver.Chrome(\"/Users/user/Desktop/chromedriver\")\n",
    "driver.implicitly_wait(3)\n",
    "driver.get(url)\n",
    "time.sleep(1)\n",
    "\n",
    "# http://bongholee.com/2017/06/python-web-crawling%EC%9D%84-%ED%86%B5%ED%95%B4-raw-data-%EA%B5%AC%ED%95%98%EA%B8%B0-selenium-library/\n",
    "elem = driver.find_element_by_tag_name(\"body\")\n",
    "no_of_pagedowns = 1\n",
    "\n",
    "while no_of_pagedowns:\n",
    "    elem.send_keys(Keys.PAGE_DOWN)\n",
    "    time.sleep(0.2)\n",
    "    no_of_pagedowns -= 1\n",
    "    \n",
    "# html = driver.page_source\n",
    "# print(html)\n",
    "# soup = BeautifulSoup(html, \"html.parser\")\n",
    "# date_list = soup.find_all(\"time\", class_=\"day\")\n",
    "# content_list = soup.find_all(\"span\", class_=\"text\")\n",
    "# len(date_list)\n",
    "# print(date.text)\n",
    "# print(content.text)\n",
    "\n",
    "date_list = driver.find_elements_by_xpath(\"//time[@class='day']\")\n",
    "content_list = driver.find_elements_by_xpath(\"//span[@class='text']\")\n",
    "\n",
    "for date in date_list:\n",
    "    print(date.text)\n",
    "\n",
    "for content in content_list:\n",
    "    print(content.text)\n",
    "\n",
    "# len(date_list)\n",
    "# len(content_list)\n",
    "                                       \n",
    "# driver.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# error code(해석 불가)\n",
    "# https://paulhoganreid.wordpress.com/2015/01/19/using-python-and-selenium-to-scrape-an-infinitely-scrolling-table/\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import selenium.webdriver\n",
    "\n",
    "url = \"https://channels.vlive.tv/F51143/fan\" \n",
    "driver = webdriver.Chrome(\"/Users/user/Desktop/chromedriver\")\n",
    "driver.get(url)\n",
    "\n",
    "while driver.find_element_by_xpath(\"//time[@class='day']\").text != '2019. 08. 28.':\n",
    "    elem = driver.find_element_by_tag_name('time')\n",
    "    elem.send_keys(Keys.PAGE_DOWN)\n",
    "    \n",
    "date_list = driver.find_elements_by_xpath(\"//time[@class='day']\")\n",
    "content_list = driver.find_elements_by_xpath(\"//span[@class='text']\")\n",
    "\n",
    "print(len(date_list))\n",
    "print(len(content_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "\n",
    "url = \"https://channels.vlive.tv/F51143/fan\" \n",
    "driver = webdriver.Chrome(\"/Users/user/Desktop/chromedriver\")\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'web_fetch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-584facf7ba79>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"https://channels.vlive.tv/F51143/fan\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# res = requests.get(url)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweb_fetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'web_fetch' is not defined"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "import requests\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://channels.vlive.tv/F51143/fan\"\n",
    "res = requests.get(url)\n",
    "data = res.decode('utf-8')\n",
    "dataset = json.loads(data)\n",
    "print(dataset)\n",
    "# data = json.loads(res.content.decode('utf-8'))\n",
    "# print(data)\n",
    "\n",
    "# text = res.json()\n",
    "\n",
    "# pprint.pprint(text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
