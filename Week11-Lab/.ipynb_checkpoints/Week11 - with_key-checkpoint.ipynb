{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 11 - Social Data Mining 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Twitter 데이터 수집하기\n",
    "\n",
    "트위터는 데이터를 수집할 수 있는 API를 제공한다. API는 크게 다음과 같은 두가지로 제공된다.\n",
    "* REST API - 주어진 query에 맞는 데이터를 제공한다.\n",
    "* Streaming API - 제시된 키워드 등이 포함된 데이터를 streaming 해준다.\n",
    "\n",
    "트위터 API에 대응하는 여러 라이브러리가 있지만, 본 예제에서는 tweepy 를 사용한다. tweepy의 설치는 다음과 같다.\n",
    "* pip install tweepy\n",
    "\n",
    "트위터는 OAuth 인증을 통해 데이터에 접근이 가능하다. OAuth 인증을 하기 위해서는 트위터 개발자 사이트에 등록하고, 데이터 수집을 위한 앱을 등록해야 한다. 등록을 마치면 OAuth 인증을 위한 키가 제공되는데 해당 키를 이용하여 트위터 API를 사용할 수 있다.\n",
    "\n",
    "(강의 슬라이드 참고)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### tweepy 및 OAuth 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "\n",
    "# OAuth setup\n",
    "consumer_key = ''\n",
    "consumer_secret = ''\n",
    "access_token = ''\n",
    "access_secret = ''\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_secret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tweepy api 오브젝트 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = tweepy.API(auth) # initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_timeline = api.home_timeline()\n",
    "my_timeline\n",
    "\n",
    "# for tweet in my_timeline:\n",
    "#     print(tweet.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 특정 사용자의 타임라인 수집"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user = api.get_user(\"u_simin\")\n",
    "user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user.id\n",
    "user.description\n",
    "user.created_at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user = api.get_user(102026929)  # 만약 screen_name이 아닌 id를 알고 있다면"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_timeline = api.user_timeline(user.id)\n",
    "# print(user_timeline[0])\n",
    "tweets = []\n",
    "for tweet in user_timeline:\n",
    "    tweets.append(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets[1].text\n",
    "tweets[1].user\n",
    "tweets[1].user.name\n",
    "tweets[1]._json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 특정 유저의 친구 목록"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user = api.get_user(\"Yunaaaa\")\n",
    "user.screen_name\n",
    "user.followers_count # 너무 많아서 못함\n",
    "user.friends_count # 김연아가 팔로워한 사람들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for friend in user.friends():\n",
    "    print(friend.screen_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(주의) user.followers()는 모두 665714 명이다. 트위터 API는 15분간 15개의 request call을 허용한다. 한번의 request 마다 20개의 followers를 가져온다면 모두 약 44381여개의 call이 필요하기 때문에 rate limit을 넘어서게 된다. 따라서 이 경우는 15개를 가져오고 잠시 쉬었다가 다시 가져오는 등의 방법을 사용한다. 처리 방법은 뒤에서 다루고 여기서는 call을 많이 사용하지 않는 friends의 리스트를 가져와 보았다.\n",
    "\n",
    "https://dev.twitter.com/rest/public/rate-limiting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pagination and Cursor\n",
    "\n",
    "트위터의 정보는 pagination 되어 있다. page를 넘겨가며 데이터를 수집하기 위해서는 cursor 를 사용한다. Cursor는 다음과 같이 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tweets = []\n",
    "for tweet in tweepy.Cursor(api.user_timeline, id=\"u_simin\").items(10): # pagination한 것 중에, 10개의 데이터만 수집\n",
    "    my_tweets.append(tweet)\n",
    "    \n",
    "my_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tweets[0].text\n",
    "my_tweets[1].user.url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "수집하고자 하는 트윗의 갯수를 지정할 수 있다. 이 경우 아이템의 숫자 (items()) 혹은 페이지의 숫자 (pages()) 를 지정한다.\n",
    "\n",
    "```\n",
    "# Only iterate through the first 200 statuses\n",
    "for status in tweepy.Cursor(api.user_timeline).items(200):\n",
    "    process_status(status)\n",
    "\n",
    "# Only iterate through the first 3 pages\n",
    "for page in tweepy.Cursor(api.user_timeline).pages(3):\n",
    "    process_page(page)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling the rate limit using cursors\n",
    "\n",
    "위에서 언급한 것처럼, 많은 양의 트윗을 수집하고자 할 때, **```RateLimitError```** 에러가 발생한다. 이런 경우 페이지를 넘기기 전에 (next()) 에러를 체크하고, **```RateLimitError```** 에러가 발생하면 그 만큼 수집을 멈추었다가 재개하여야 한다. 에러 핸들링을 하지 않으면 프로그램이 멈추고 데이터 수집도 멈춘다.\n",
    "\n",
    "```\n",
    "def limit_handled(cursor):\n",
    "    while True:\n",
    "        try:\n",
    "            yield cursor.next()\n",
    "        except tweepy.RateLimitError:\n",
    "            time.sleep(15 * 60)\n",
    "\n",
    "for follower in limit_handled(tweepy.Cursor(api.followers).items()):\n",
    "    process_follower(follower)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming Tweet Data\n",
    "\n",
    "Streaming API는 rate limit의 제한이 없어 트윗을 수집할 때 유용하다. 그러나 모든 트윗을 제공하는 것은 아니고, 전체 트윗의 1%만을 random 으로 제공해 준다.\n",
    "\n",
    "Streaming API를 사용하여 트윗을 수집하려면 다음과 같은 순서를 따른다.\n",
    "\n",
    "* StreamListener 클래스를 상속받은 lister 클래스를 만든다.\n",
    "* Stream 오브젝트를 생성한다.\n",
    "* Stream 오브젝트에 Twitter API를 연결한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. StreamListener 클래스 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "\n",
    "class MyListener(tweepy.StreamListener): # tweepy의 StreamListener class를 상속받음\n",
    "    \n",
    "    def on_status(self, data): # on_data가 on_status에 data를 넘겨줌\n",
    "        print(data.text + \"\\n----\")\n",
    "    \n",
    "# 이해 안 가도, 그냥 가져다 쓰세요~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Stream 오브젝트 생성하고 Twitter API 연결 (OAuth 설정)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_stream = tweepy.Stream(auth, MyListener())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Streaming 시작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_stream.filter(track=['trump', 'korea'])\n",
    "# jupyter는 메모리관리에 썩 좋지 않음. kernel -> interrupt 해줘야 함. 터미널에서 돌려보도록 하세요!\n",
    "# '트럼프' 키워드와 '수능' 키워드 올라오는 속도가 다름"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. json 파일로 저장할 수 있도록 MyListener 클래스 수정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyListener(tweepy.StreamListener):\n",
    "    \n",
    "    def on_data(self, data):\n",
    "        try:\n",
    "            with open('tweet_stream.json', 'a') as file: # 'a' = append\n",
    "                file.write(data) # 여태 수집한 data들을 수집\n",
    "                print(data) # 모니터링 \n",
    "                return True \n",
    "        except BaseException as e: \n",
    "            print(\"Error on_data: {}\".format(str(e)))\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_stream = tweepy.Stream(auth, MyListener())\n",
    "twitter_stream.filter(track=['trump', 'korea'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "(참고1) ```on_status vs. on_data```\n",
    "\n",
    "```on_status```는 데이터를 tweepy object 형태로 return한다. 따라서 data.text와 같은 notation이 가능하다. 하지만 ```on_data```는 데이터를 str로 return 한다. 우리는 데이터를 .json 파일로 저장할 것이기 때문에 tweepy object보다는 str로 저장하여야 한다. 따라서 두번째 수정된 코드에서는 ```on_data```를 사용하였다.\n",
    "\n",
    "(참고2) ```on_status vs. on_data```\n",
    "\n",
    "두개의 메소드는 StreamListener에 구현되어 있다. 클래스를 상속 받으면 부모 클래스에 구현된 메소드를 재정의(override)해서 사용할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 터미널에서 프로그램 실행\n",
    "\n",
    "jupyter notebook이 편리하긴 하지만 어떤 경우에는 터미널에서 프로그램을 실행하여야 한다. 스트리밍한 데이터를 저장하는 프로그램은 계속 데이터를 받아오기 때문에 jupyter notebook에서 실행하면 메모리 로드가 많아 작동불능 상태에 빠지기 쉽다. 따라서 이 경우 터미널에서 프로그램을 실행시킨다.\n",
    "\n",
    "* 첫번째 스트리밍 코드는 twitter-stream.py 로 저장되어 있다.\n",
    "* 수정된 두번째 스트리밍 코드는 twitter-stream-save.py로 저장되어 있다.\n",
    "\n",
    "실행은 터미널에서 다음과 같이 한다.\n",
    "\n",
    "* python twitter-stream-save.py\n",
    "\n",
    "프로그램의 종료는 CTRL-C 를 누른다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON 파일 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "json_data = []\n",
    "with open(\"tweet_stream.json\") as file:\n",
    "    data = file.readlines()\n",
    "    for d in data:\n",
    "        json_data.append(json.loads(d)) # dict file 구축\n",
    "        # json.loads: json 형식의 string을 python dictionary 형식으로 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for tweet in json_data:\n",
    "    if (\"text\" in tweet) and (\"user\" in tweet):\n",
    "        print(str(i) + \" - \" + tweet[\"user\"][\"name\"] + \" :: \" + tweet[\"text\"])\n",
    "        print(\"---\")\n",
    "        i = i + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2. OpenAPI 를 이용하여 데이터 크롤링\n",
    "\n",
    "지금까지 살펴본 Twitter나 Facebook은 해당 서비스의 데이터 크롤링을 위한 파이썬 라이브러리를 이용하여 데이터를 수집할 수 있었다. 이들 라이브러리는 Twitter나 Facebook이 제공하는 OpenAPI에 맞게 개발되었다. 모든 서비스를 위한 라이브러리가 제공되는지는 않기 때문에 특정 서비스의 OpenAPI를 이용하려면 OpenAPI가 제공하는 방식에 맞게 프로그램을 설계하고 데이터를 수집한다.\n",
    "\n",
    "대개 OpenAPI는 다음과 같은 형식의 URL을 이용하여 필요한 자료를 API 서버에 request 한다. (call)\n",
    "* (포맷) http://web-address/api-name?param1=xxx&param2=xxx&param3=xxx\n",
    "* (사례) http://api.openweathermap.org/data/2.5/weather?q=Seoul&units=metric&appid=yourkey\n",
    "\n",
    "API서버가 call을 받으면 다음과 같이 JSON 포맷으로 결과를 반환한다. (response)\n",
    "```\n",
    "{\n",
    "    \"coord\":{\"lon\":126.98,\"lat\":37.57},\n",
    "    \"weather\":[\n",
    "         {\"id\":701,\"main\":\"Mist\",\"description\":\"mist\",\"icon\":\"50d\"},\n",
    "         {\"id\":500,\"main\":\"Rain\",\"description\":\"light rain\",\"icon\":\"10d\"},   \n",
    "         {\"id\":721,\"main\":\"Haze\",\"description\":\"haze\",\"icon\":\"50d\"}\n",
    "    ],\n",
    "    \"base\":\"stations\",\n",
    "    \"main\":{\"temp\":11.01,\"pressure\":1022,\"humidity\":43,\"temp_min\":10,\"temp_max\":12},\n",
    "    \"visibility\":10000,\n",
    "    \"wind\":{\"speed\":3.1,\"deg\":360},\n",
    "    \"clouds\":{\"all\":90},\n",
    "    \"dt\":1541917800,\n",
    "    \"sys\":{\"type\":1,\"id\":7668,\"message\":0.0054,\"country\":\"KR\",\"sunrise\":1541887637,\"sunset\":1541924664},\n",
    "    \"id\":1835848,\n",
    "    \"name\":\"Seoul\",\n",
    "    \"cod\":200\n",
    "}\n",
    "```\n",
    "\n",
    "이와 같이 API를 사용하기 위해서는 공개된 OpenAPI라 하더라도 개발자로 등록을 해야 한다.\n",
    "\n",
    "개발자로 등록을 한 후에는 데이터 수집을 위해 제작할 application을 등록한다.\n",
    "\n",
    "application을 등록하면 보통 app-key라는 것을 주는데, 이것은 일종의 아이디-패스워드이다. 즉, 누가 접속을 해서 데이터를 수집해가는 지를 서버에 알려주는 역할을 하며, 또한 서버 입장에서 데이터를 수집하는 앱의 트래픽을 콘트롤하기도 한다. (대개의 경우 call 숫자가 정해져 있다.)\n",
    "\n",
    "본 예제에서는 openweathermap.org에서 제공하는 공개 데이터를 사용해 보고자 한다. 개발자 등록과 앱 등록은 슬라이드를 참고하자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 앱 기본 설정\n",
    "\n",
    "app_key와 base_url 등 기본 설정을 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://api.openweathermap.org/data/2.5/weather?q=Seoul&units=metric&appid=781da3bc7b2aeb07cebe04999a760842\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import json\n",
    "   \n",
    "app_key  = \"781da3bc7b2aeb07cebe04999a760842\"\n",
    "loc      = \"Seoul\"\n",
    "base_url = \"http://api.openweathermap.org/data/2.5/weather?q={}&units=metric&appid={}\".format(loc, app_key)\n",
    "\n",
    "print(base_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 현재 날씨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data = \"\"\n",
    "with urllib.request.urlopen(base_url) as response:\n",
    "    weather_data = json.loads(response.read().decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 데이터 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'coord': {'lon': 126.98, 'lat': 37.57},\n",
       " 'weather': [{'id': 801,\n",
       "   'main': 'Clouds',\n",
       "   'description': 'few clouds',\n",
       "   'icon': '02d'}],\n",
       " 'base': 'stations',\n",
       " 'main': {'temp': 14.9,\n",
       "  'pressure': 1013,\n",
       "  'humidity': 54,\n",
       "  'temp_min': 14,\n",
       "  'temp_max': 16},\n",
       " 'visibility': 10000,\n",
       " 'wind': {'speed': 4.1, 'deg': 290, 'gust': 6.7},\n",
       " 'clouds': {'all': 20},\n",
       " 'dt': 1573455957,\n",
       " 'sys': {'type': 1,\n",
       "  'id': 5501,\n",
       "  'country': 'KR',\n",
       "  'sunrise': 1573423609,\n",
       "  'sunset': 1573460720},\n",
       " 'timezone': 32400,\n",
       " 'id': 1835848,\n",
       " 'name': 'Seoul',\n",
       " 'cod': 200}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 반환되는 날씨 데이터의 정보는 다음을 참고해서 해석하자.\n",
    "https://openweathermap.org/current\n",
    "https://openweathermap.org/weather-conditions\n",
    "\n",
    "(참고) `&lang=kr`를 URL에 넣어주면 한글로 데이터를 반환한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "습도: 54%\n",
      "현재기온: 14.9도\n",
      "구름이 20% 덮여있고 풍속은 4.1m/s입니다.\n",
      "현재 가시거리는 10.0km입니다.\n"
     ]
    }
   ],
   "source": [
    "humidity = weather_data[\"main\"][\"humidity\"]\n",
    "temp = weather_data[\"main\"][\"temp\"]\n",
    "cloud = weather_data[\"clouds\"][\"all\"]\n",
    "wind = weather_data[\"wind\"][\"speed\"]\n",
    "visibility = weather_data[\"visibility\"]\n",
    "print(\"습도: {}%\".format(humidity))\n",
    "print(\"현재기온: {}도\".format(temp))\n",
    "print(\"구름이 {}% 덮여있고 풍속은 {}m/s입니다.\".format(cloud, wind))\n",
    "print(\"현재 가시거리는 {}km입니다.\".format(visibility/1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UNIX Timestamp\n",
    "\n",
    "유닉스 운영체계에서 자체적으로 시각을 나타내기 위해 사용하는 포맷을 Unix Timestamp 라고 한다. POSIX 나 Epoch 라고 부르기도 한다. 1970년 1월 1일 0시 0분 0초 (UTC: 협정 세계시) 부터 시작하고 음수로 표현되면 그 이전 시간을 의미한다. 32비트로 표현하며 1970년 1월 1일 부터 몇초가 지났는지를 표현한다.\n",
    "\n",
    "* 예: 1541922737 seconds since Jan 01 1970. (UTC) \n",
    "* https://www.unixtimestamp.com/\n",
    "\n",
    "UNIX Timestamp 를 변환하는 방법은 다음과 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-11 07:06:49\n",
      "2019-11-11 17:25:20\n",
      "오늘 일출시간은 07시 06분이고, 일몰시간은 17시 25분입니다.\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "sunrise = weather_data[\"sys\"][\"sunrise\"]\n",
    "sunset = weather_data[\"sys\"][\"sunset\"]\n",
    "\n",
    "print(datetime.fromtimestamp(sunrise).strftime('%Y-%m-%d %H:%M:%S'))\n",
    "print(datetime.fromtimestamp(sunset).strftime('%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "print(\"오늘 일출시간은 {}이고, 일몰시간은 {}입니다.\".format(\n",
    "    datetime.fromtimestamp(sunrise).strftime('%H시 %M분'),\n",
    "    datetime.fromtimestamp(sunset).strftime('%H시 %M분')))\n",
    "\n",
    "# locale 에러 시 strftime(~).encode().decode(\"unicode-escape\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 미세먼지 정보"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "미세먼지 정보를 가져와보자.\n",
    "\n",
    "미세먼지와 관련된 정보는 다음의 API를 활용할 예정이다.\n",
    "\n",
    "* http://data.seoul.go.kr/dataList/datasetView.do?infId=OA-1204&srvType=A&serviceKind=1\n",
    "\n",
    "데이터 수집을 위해서는 인증키(위의 App Key와 유사)을 받아야 하는데, 인증키 신청 버튼을 눌러 발급받도록 하자. 몇가지 form을 작성하면 키가 발급된다.\n",
    "\n",
    "도큐멘테이션에 나와았는 API의 사용은 다음과 같다.\n",
    "\n",
    "* http://openAPI.seoul.go.kr:8088/(인증키)/xml/ForecastWarningMinuteParticleOfDustService/1/1/ \n",
    "\n",
    "위의 URL의 (인증키) 부분에는 발급받은 인증키를 넣도록 하자.\n",
    "\n",
    "현재 이 URL은 XML 형태로 데이터를 제공한다. xml을 json으로 바꿔서 json 형식으로 데이터를 받아보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://openAPI.seoul.go.kr:8088/781da3bc7b2aeb07cebe04999a760842/json/ForecastWarningMinuteParticleOfDustService/1/1/ \n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import json\n",
    "   \n",
    "mykey  = \"\" # 이곳의 key를 발급받으세요\n",
    "base_url = \"http://openAPI.seoul.go.kr:8088/{}/json/ForecastWarningMinuteParticleOfDustService/1/1/ \".format(mykey)\n",
    "\n",
    "# print(base_url)\n",
    "fine_dust_data = \"\"\n",
    "# with urllib.request.urlopen(base_url) as response:\n",
    "#     fine_dust_data = json.loads(response.read().decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'json'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fine_dust_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-8e1944210bde>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtime_observation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrptime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfine_dust_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ForecastWarningMinuteParticleOfDustService\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"row\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"APPLC_DT\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"%Y%m%d%H%M\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdust_grade\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfine_dust_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ForecastWarningMinuteParticleOfDustService\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"row\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"CAISTEP\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdust_grade\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: string indices must be integers"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "time_observation = datetime.strptime(fine_dust_data[\"ForecastWarningMinuteParticleOfDustService\"][\"row\"][0][\"APPLC_DT\"], \"%Y%m%d%H%M\")\n",
    "dust_grade = fine_dust_data[\"ForecastWarningMinuteParticleOfDustService\"][\"row\"][0][\"CAISTEP\"]\n",
    "dust_grade\n",
    "\n",
    "# · 0∼30 : 좋음 · 31∼80 : 보통 · 81∼150 : 나쁨 · 151 이상 : 매우 나쁨 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dust_grade' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-c7126124e182>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mwarning\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0mdust_grade\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"매우 나쁨\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mwarning\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"외출을 자제하시는 것이 좋습니다.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32melif\u001b[0m \u001b[0mdust_grade\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"나쁨\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mwarning\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"외출시 꼭 마스크를 착용하세요.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dust_grade' is not defined"
     ]
    }
   ],
   "source": [
    "warning = \"\"\n",
    "if dust_grade == \"매우 나쁨\":\n",
    "    warning = \"외출을 자제하시는 것이 좋습니다.\"\n",
    "elif dust_grade == \"나쁨\":\n",
    "    warning = \"외출시 꼭 마스크를 착용하세요.\"\n",
    "elif dust_grade == \"보통\":\n",
    "    warning = \"마음껏 외출하세요.\"\n",
    "else:\n",
    "    warning = \"너무 좋은 날씨네요. 마음껏 외출하세요 :)\"\n",
    "message = \"{}년 {}월 {}일 {}시 현재, 미세먼지 수준은 {}입니다. \".format(time_observation.year, time_observation.month, time_observation.day, time_observation.hour, dust_grade)\n",
    "\n",
    "print(message + warning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 4. Advanced Web Crawling\n",
    "\n",
    "지난 시간에 다뤘던 것처럼, BeautifulSoup을 이용하여 HTML 내의 특정 요소를 찾아 데이터를 추출할 수 있다. 그러나 최근에 만들어지는 웹사이트의 경우, AJAX 를 사용하여 데이터를 불러오는 경우가 많아, 필요한 데이터를 추출하기 어려운 경우가 있다. 이런 경우, 브라우저에서 제공하는 developer tool 을 사용하여 데이터 소스를 찾아 직접 데이터를 다운로드 하기도 한다. (슬라이드 참조)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 뉴스 사이트 (media daum)에서 기사의 본문을 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "\n",
    "url = \"https://news.v.daum.net/v/20181111190005301\"\n",
    "doc = \"\"\n",
    "with urllib.request.urlopen(url) as url:\n",
    "    doc = url.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(doc, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_article = soup.find_all(\"div\", class_=\"article_view\")\n",
    "news_article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_article[0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 뉴스사이트에서 기사의 댓글을 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = soup.find_all(\"ul\", class_=\"list_comment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위에서 기사의 본문을 추출한 것과 동일한 방법으로 기사의 댓글을 추출하고자 하였다. 브라우저의 inspector에서 보았을 때 해당 블락은 ```<ul class=\"list_comment\">```로 정의되어 있었다. 그러나 실제로 해당 요소는 검색이 되지 않았다. 이는 페이지 소스에 이 요소가 포함되지 않았기 때문이다. 해당 요소는 page 가 모두 로딩된 후, javascript를 이용해서 페이지에 **삽입**된다. 따라서 이런 경우 데이터 수집이 불가능하다.\n",
    "\n",
    "삽입되는 해당요소가 무엇인지를 찾기 위해서는 브라우저가 제공하는 개발자 도구에서 해당 요소를 삽입하는 코드를 찾아야 한다.\n",
    "(슬라이드)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON으로 댓글 추출하기\n",
    "\n",
    "추출한 댓글 삽입 코드의 주소: https://comment.daum.net/apis/v1/posts/36628933/comments?parentId=0&offset=0&limit=3&sort=RECOMMEND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 308637472,\n",
       "  'userId': 21182555,\n",
       "  'postId': 36628933,\n",
       "  'forumId': -99,\n",
       "  'parentId': 0,\n",
       "  'type': 'COMMENT',\n",
       "  'status': 'S',\n",
       "  'flags': 256,\n",
       "  'content': '도대체 법을 만든다는 놈들\\n대가리속엔 뭐가들었는지...',\n",
       "  'createdAt': '2018-11-11T19:26:52+0900',\n",
       "  'updatedAt': '2018-11-11T19:26:52+0900',\n",
       "  'childCount': 41,\n",
       "  'likeCount': 2128,\n",
       "  'dislikeCount': 87,\n",
       "  'recommendCount': 2041,\n",
       "  'user': {'id': 21182555,\n",
       "   'status': 'S',\n",
       "   'type': 'USER',\n",
       "   'flags': 0,\n",
       "   'icon': 'https://t1.daumcdn.net/profile/P8ky.gyBV9I0',\n",
       "   'url': '',\n",
       "   'username': 'DAUM:Cjzb9',\n",
       "   'roles': 'ROLE_USER,ROLE_DAUM,ROLE_IDENTIFIED',\n",
       "   'providerId': 'DAUM',\n",
       "   'providerUserId': 'Cjzb9',\n",
       "   'displayName': '수구꼴통 참수',\n",
       "   'description': '',\n",
       "   'commentCount': 4583}},\n",
       " {'id': 308637027,\n",
       "  'userId': 20543505,\n",
       "  'postId': 36628933,\n",
       "  'forumId': -99,\n",
       "  'parentId': 0,\n",
       "  'type': 'COMMENT',\n",
       "  'status': 'S',\n",
       "  'flags': 4352,\n",
       "  'content': '100만원씩 합시다. 1000만원도 좋고.',\n",
       "  'createdAt': '2018-11-11T19:24:47+0900',\n",
       "  'updatedAt': '2018-11-11T19:24:47+0900',\n",
       "  'childCount': 58,\n",
       "  'likeCount': 1737,\n",
       "  'dislikeCount': 308,\n",
       "  'recommendCount': 1429,\n",
       "  'user': {'id': 20543505,\n",
       "   'status': 'S',\n",
       "   'type': 'USER',\n",
       "   'flags': 0,\n",
       "   'icon': 'https://k.kakaocdn.net/dn/KMfJp/btqzH0M4DuS/YS2SkuLQXZ4dK96OXSLbF1/img_110x110.jpg',\n",
       "   'username': 'KAKAO:563286464',\n",
       "   'roles': 'ROLE_USER,ROLE_KAKAO,ROLE_IDENTIFIED',\n",
       "   'providerId': 'KAKAO',\n",
       "   'providerUserId': '25864808',\n",
       "   'displayName': '한만형',\n",
       "   'description': '',\n",
       "   'commentCount': 1090}},\n",
       " {'id': 308638072,\n",
       "  'userId': -18247746,\n",
       "  'postId': 36628933,\n",
       "  'forumId': -99,\n",
       "  'parentId': 0,\n",
       "  'type': 'COMMENT',\n",
       "  'status': 'S',\n",
       "  'flags': 256,\n",
       "  'content': '멀쩡한 놈들이 상습적으로\\n하는경우가 허다하다\\n백만원으로 올리자',\n",
       "  'createdAt': '2018-11-11T19:29:34+0900',\n",
       "  'updatedAt': '2018-11-11T19:29:34+0900',\n",
       "  'childCount': 21,\n",
       "  'likeCount': 1074,\n",
       "  'dislikeCount': 111,\n",
       "  'recommendCount': 963,\n",
       "  'user': {'id': -18247746,\n",
       "   'status': 'S',\n",
       "   'type': 'USER',\n",
       "   'flags': 0,\n",
       "   'icon': 'https://t1.daumcdn.net/profile/EqtYGdqF3GU0',\n",
       "   'url': '',\n",
       "   'username': 'DAUM:1ez4u',\n",
       "   'roles': 'ROLE_USER,ROLE_DAUM,ROLE_IDENTIFIED',\n",
       "   'providerId': 'DAUM',\n",
       "   'providerUserId': '1ez4u',\n",
       "   'displayName': '하늘',\n",
       "   'description': '',\n",
       "   'commentCount': 734}}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "json_url = \"https://comment.daum.net/apis/v1/posts/36628933/comments?parentId=0&offset=0&limit=3&sort=RECOMMEND\"\n",
    "with urllib.request.urlopen(json_url) as url:\n",
    "    json_doc = url.read().decode(\"utf-8\")\n",
    "    json_data = json.loads(json_doc)\n",
    "json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "수구꼴통 참수\n",
      "도대체 법을 만든다는 놈들\n",
      "대가리속엔 뭐가들었는지...\n",
      "2128 87\n",
      "--\n",
      "한만형\n",
      "100만원씩 합시다. 1000만원도 좋고.\n",
      "1737 308\n",
      "--\n",
      "하늘\n",
      "멀쩡한 놈들이 상습적으로\n",
      "하는경우가 허다하다\n",
      "백만원으로 올리자\n",
      "1074 111\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "for data in json_data:\n",
    "    print(data['user']['displayName'])\n",
    "    print(data['content'])\n",
    "    print(data['likeCount'], data['dislikeCount'])\n",
    "    print(\"--\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```parentId=0&offset=0&limit=3``` 의 패러미터 값을 조정하여 더 많은 데이터 수집 가능. 예를 들어 limit은 보여지는 댓글의 갯수를 얘기함.\n",
    "\n",
    "limit을 100개로 고치고 프로그램을 다시 실행해 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_url = \"https://comment.daum.net/apis/v1/posts/36628933/comments?parentId=0&offset=0&limit=100&sort=RECOMMEND\"\n",
    "with urllib.request.urlopen(json_url) as url:\n",
    "    json_doc = url.read().decode(\"utf-8\")\n",
    "    json_data = json.loads(json_doc)\n",
    "    \n",
    "for data in json_data:\n",
    "    print(data['user']['displayName'])\n",
    "    print(data['content'])\n",
    "    print(data['likeCount'], data['dislikeCount'])\n",
    "    print(\"--\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
